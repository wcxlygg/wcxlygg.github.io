<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Vision | Chunxiang Wang</title>
    <link>https://luoyeguigenno1.github.io/tag/computer-vision/</link>
      <atom:link href="https://luoyeguigenno1.github.io/tag/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    <description>Computer Vision</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 02 Mar 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://luoyeguigenno1.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Computer Vision</title>
      <link>https://luoyeguigenno1.github.io/tag/computer-vision/</link>
    </image>
    
    <item>
      <title>Automatic Tracking Zebrafish Larvae in Group</title>
      <link>https://luoyeguigenno1.github.io/project/automatic-tracking-zebrafish-larvae-in-group/</link>
      <pubDate>Mon, 02 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://luoyeguigenno1.github.io/project/automatic-tracking-zebrafish-larvae-in-group/</guid>
      <description>&lt;p&gt;Video tracking has become a standard procedure for studying model organisms&#39; functional characteristics, during which the trajectory of each animal and their corresponding movement information are extracted for a range of biomedical studies, such as genetics, drug discovery, toxicology, behavioral science, etc. Manual tracking via frame-by-frame labeling is time-consuming, subjective, and tedious, whose results are often not consistent due to the fatigue and inexperience of operators. Therefore, an automatic tracking system is indispensable, which achieves a remarkedly higher research output and provides more insightful and detailed statistics.&lt;/p&gt;
&lt;p&gt;Zebrafish larva stands out as a popular vertebrate model organism for large-scale chemical and genetic screens due to its high similarity of gene and cardiovascular system to human, optical transparency, easiness of acquisition, and rapid developmental process. Compared with other experimental organisms of zebrafish, larval motility, along with the transparent morphology, implies more phenotypic information than cells and embryos. Unlike adult zebrafish, larval relative transparency and immature nervous system allow detailed studies of gene regulation and function and neuropharmacological studies that cannot be performed in the adult period. Researchers generally transfer foreign materials into zebrafish embryos or specific organs of zebrafish larvae after which larval locomotive behaviors are recorded for further analysis. These investigations cover the fields of the analysis of movement characteristics, social behavior, and the assessment of phenotypes resulting from:  gene knockdown approaches, genetic mutations, drugs, etc.&lt;/p&gt;
&lt;p&gt;Many single and multiple object tracking systems have been developed for adult zebrafish, reporting outstanding tracking performance. However, their performance dramatically drops when applied to a larvae group, where individuals&#39; identities switch easily, and thus the statistical data is not usable. This weakeness highly limits the efficiency of relevant biomedical research. Over an extended period, tracking a larval zebrafish group is a labor-consuming manual work due to these systems’ inability to generate reliable results.&lt;/p&gt;
&lt;p&gt;It is small size, transparent and similar appearance, frequent occlusions, and discontinuous kinematics that combine to make larval tracking notoriously difficult.  Small size makes a multiple-sensor tracking system and labeling approaches infeasible due to imaging and operating constraints. Transparent and similar appearance causes tracking algorithms based on extracting superficial appearance features to fail, as the difference between larvae’s appearance is much less distinguishable than that of adult fish.&lt;/p&gt;
&lt;p&gt;Occlusion is a general, and the toughest tracking difficulty since occluded targets are recognized as a single connected component in image, and their identities may switch after the point of overlap. This issue can be corrected according to motion continuity. However, different from the adult’s continual pattern of swimming, larvae can stay static over a long period and then flick, which is referred to as oscillatory movements. This locomotive characteristic, along with larva&amp;rsquo;s abruptly switched swim modes yields to a long-lasting occlusion where larvae can change their motion stochastically. This results in the poor performance of systems that calculate the most likely assignment of identities before and after an occlusion based on continuous motion models and body geometry.&lt;/p&gt;
&lt;p&gt;Presently, confronted with these tracking difficulties for larvae, the most widely-used workaround is to segregate single larva in multiwell plates physically. Another state-of-the-art solution, the well-known idTracker.ai, takes advantage of the powerful feature extracting capability of deep learning to identify objects individually. Besides, ZebraZoom and Wang et al. use background subtraction to segment and track larvae.
Under certain circumstances, these methods work quite well, whereas it still seems that they have their limitations. Strictly limiting experiments to one zebrafish per dish constrains the research application as group behavior investigations, for example, cannot be performed. idTracker.ai is computationally and memory expensive, and requires a whole recording to train the neural network, which disables this method to fit in real-time, streaming applications. ZebraZoom and the method proposed by Wang et al. is only capable of processing a restricted number of larvae ($&amp;lt;$10) as their capability to tackle the occlusion problem is limited.&lt;/p&gt;
&lt;p&gt;This project proposed DanioSense, an efficient, robust, and high-throughput tracking system to automate the quantification of zebrafish larvae group movement. The task of tracking group larval zebrafish is to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;discover multiple objects in individual frames; &lt;br&gt;&lt;/li&gt;
&lt;li&gt;maintain the identity information across continuous frames; &lt;br&gt;&lt;/li&gt;
&lt;li&gt;yield their trajectories while recording the statistical parameters. &lt;br&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The main contributions of this work include&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;resolving the occlusion problem to obtain reliable trajectories with a light pre-trained convolutional neural network and a centerline extraction algorithm; &lt;br&gt;&lt;/li&gt;
&lt;li&gt;handling the discontinuous kinematics by an Adaptive Kalman Filter to obtain the optimal estimation of larvae’s locomotive parameters; &lt;br&gt;&lt;/li&gt;
&lt;li&gt;adopting an enriched vector instead of a point to generate more detailed morphology and movement statistics; &lt;br&gt;&lt;/li&gt;
&lt;li&gt;providing a variety of tracking visualization functions that offers users an intuitive sense of larval movements. &lt;br&gt;
In addition, DanioSense is applicable to the behavior quantification of other fish-like animals, only requiring users to upload their own datasets and retrain the convolutional neural network classifier.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There is a video which represents the performance of the algorithm here:














  


&lt;video controls &gt;
  &lt;source src=&#34;my_video.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;
&lt;p&gt;For more detailed information, please refer to this &lt;a href=&#34;https://luoyeguigenno1.github.io/files/DanioSense%20Automated%20High-throughput%20Quantification%20of%20Zebrafish%20Larvae%20Group%20Movement.pdf&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt;, which is submitted to &lt;a href=&#34;https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8856&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE Transactions on Automation Science and Engineering&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Supervisor: Prof. &lt;a href=&#34;https://scholar.google.com.hk/citations?user=2DdpHLEAAAAJ&amp;amp;hl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Huijun Gao&lt;/a&gt;,  IEEE Fellow and Professor of Department of Automation, Harbin Institute of Technology&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Zebrafish Larva Positioning System</title>
      <link>https://luoyeguigenno1.github.io/project/zebrafish-larva-positioning-system/</link>
      <pubDate>Mon, 02 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://luoyeguigenno1.github.io/project/zebrafish-larva-positioning-system/</guid>
      <description>&lt;p&gt;Positioning the larva inside a micropipette accurately and robustly plays a pivotal role in the transportation process. The objective of this system is to transfer a larva from one location to another and adjust its posture for micromanipulation, organ imaging, drug-specific phenotypic observation, etc. A well-known relevant system is &lt;a href=&#34;https://www.unionbio.com/vast/&#34;&gt;VAST BioImager&lt;/a&gt;, which is a modular and expandable platform that is convenient to integrate with many microscopes. A typical application of VAST BioImager is to model the skeleton bones of zebrafish larvae in a super fast speed of several seconds.&lt;/p&gt;

&lt;p&gt;The setup and workflow of this system are illustrated as follows:
&lt;figure&gt;&lt;img src=&#34;1.jpg&#34; alt=&#34;Device and schematic&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Calibration. Compute the transformation matrix with the gradient-descent-based parametric identification.
&lt;figure&gt;&lt;img src=&#34;c1.jpg&#34; alt=&#34;c1&#34;&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;li&gt;Detect the object, analyse its posture.&lt;/li&gt;
&lt;li&gt;Drive the mechanical device to adjust the object to the ideal orientation.&lt;/li&gt;
&lt;li&gt;Inhale the larva into the pipette and position it to the desired position.&lt;/li&gt;
&lt;li&gt;The micromanipulator moves the pipette to the releasing position.&lt;/li&gt;
&lt;li&gt;Release larva.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The difficulties include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Abrupt motion in the process of aspiration or dispensing, as illustrated in the following video:














  


&lt;video controls &gt;
  &lt;source src=&#34;1.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/li&gt;
&lt;li&gt;Sensitive dynamic characteristics when positioning the object, shown in the video below:














  


&lt;video controls &gt;
  &lt;source src=&#34;2.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/li&gt;
&lt;li&gt;Difficulties for video tracking, including the abruptive motion, objects that are out of focus and blocked up, etc.
&lt;figure&gt;&lt;img src=&#34;3.png&#34; alt=&#34;3&#34;&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To deal with the object’s abrupt motion, defocus, occlusion problems, a fast and reliable tracker is designed, achieving a processing velocity of over 100 frames per second and an locating accuracy within 5 pixels.&lt;/p&gt;

&lt;p&gt;To model the background, an adapted Gaussian Mixture model is adopted, with equations as follows:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[p\left( {\vec x|{\chi _T},BG + FG} \right) = \sum\limits_{m = 1}^M {{\pi _m}} {\rm{N}}\left( {\vec x;\overrightarrow {{\mu _m}} ,\sigma _m^2*I} \right) \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{array}{*{20}{c}}
{\pi _m^\prime  = {\pi _m} + \alpha \left( {O_m^t - {\pi _m}} \right)}\\
{\vec \mu _m^\prime  = {{\vec \mu }_m} + O_m^t\left( {\alpha /{\pi _m}} \right){{\vec \delta }_m}}\\
{\sigma _m^{{2^\prime }} = \sigma _m^2 + O_m^t\left( {\alpha /{\pi _m}} \right)\left( {\vec \delta _m^T{\delta _m} - \sigma _m^2} \right)}
\end{array}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;After that, subtract the frame from the background, and then the Otsu thresholding method, along with morphological operations, is performed to segment the whole contour. With the contour, the larval position is calculated, shown in the video below.














  


&lt;video controls &gt;
  &lt;source src=&#34;3.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;

&lt;p&gt;To model this system, the force analysis is conducted as follows:
&lt;figure&gt;&lt;img src=&#34;model.jpg&#34; alt=&#34;model&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{array}{*{20}{l}}
{m{{\ddot x}_w} + b{{\dot x}_w} + G\sin \theta  = {P_0}{S_{pip}} - \frac{{{P_{air,0}}{V_{air,0}}}}{{{V_{air,0}} + u{S_{pis}} - {x_w}{S_{pip}}}}{S_{pip}}}\\
{m{{\ddot x}_w} + b{{\dot x}_w} + G\sin \theta  = \frac{{{P_0}{S_{pip}}{V_{air,0}} + {P_0}{S_{pip}}{S_{pis}}u - {P_0}S_{pip}^2{x_w} - {P_{air,0}}{V_{air,0}}{S_{pip}}}}{{{V_{air,0}} + u{S_{pis}} - {x_w}{S_{pip}}}}}\\
{m{{\ddot x}_w} + b{{\dot x}_w} + G\sin \theta  = \frac{{{P_0}{S_{pip}}{V_{air,0}} + {P_0}{S_{pip}}{S_{pis}}u - {P_0}S_{pip}^2{x_w} - {P_{air,0}}{V_{air,0}}{S_{pip}}}}{{{V_{air,0}}}}}\\
{m{{\ddot x}_w} + b{{\dot x}_w} + \frac{{{P_0}S_{pip}^2}}{{{V_{air,0}}}}{x_w} + G\sin \theta  = \frac{{{P_0}{S_{pip}}{S_{pis}}}}{{{V_{air,0}}}}u + \left( {{P_0} - {P_{air,0}}} \right){S_{pip}}}
\end{array}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Build the dynamic model:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{array}{*{20}{l}}
{m\ddot x + K\ddot x + \frac{K}{b}\frac{{{P_0}S_{pip}^2}}{{{V_{air,0}}}}\left( {\frac{{m\ddot x + G\sin \theta }}{K} + \dot x} \right) = \frac{{K{P_0}{S_{pp}}{S_{pis}}}}{{b{V_{air,0}}}}u}\\
{m\ddot x + K\ddot x + \frac{K}{b}\frac{{{P_0}S_{pip}^2}}{{{V_{air,0}}}}\frac{m}{K}\ddot x + \frac{K}{b}\frac{{{P_0}S_{pip}^2}}{{{V_{air,0}}}}\frac{{G\sin \theta }}{K} + \frac{K}{b}\frac{{{P_0}S_{pip}^2}}{{{V_{air,0}}}}\dot x = \frac{{K{P_0}{S_{pip}}{S_{pis}}}}{{b{V_{air,0}}}}u}\\
{m\ddot x + \left( {\frac{{m{P_0}S_{pp}^2}}{{b{V_{air,0}}}} + K} \right)\ddot x + \frac{{K{P_0}S_{pip}^2}}{{b{V_{air,0}}}}\dot x + \frac{{{P_0}S_{pip}^2G\sin \theta }}{{b{V_{ijr,0}}}} = \frac{{K{P_0}{S_{pip}}{S_{pis}}}}{{b{V_{air,0}}}}u}
\end{array}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Abbreviate it in the fllowing form:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[{a_1}\ddot x + {a_2}\ddot x + {a_3}\dot x + {a_4} = bu\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The parameters of the dynamic model are identified by the frequency response test, after which the PID control algorithm is desinged, achieving the following control effect:














  


&lt;video controls &gt;
  &lt;source src=&#34;4.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;

&lt;p&gt;This system manages to position a larva to a desired position in the inclined micropipette for larva transfer and manipulation, which could hardly be achieved manually. Presently, the PID control effect is not optimal for larvae with different ages. Thus, I have been working on using the Adaptive Robust control to enhance the control performance, and extend its application to other organisms, such as embryo. I plan to write a paper and submit it to IEEE Transactions on Automation Science and Engineering.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Supervisor: Prof. &lt;a href=&#34;https://scholar.google.com.hk/citations?user=2DdpHLEAAAAJ&amp;amp;hl&#34;&gt;Huijun Gao&lt;/a&gt;,  IEEE Fellow and Professor of Department of Automation, Harbin Institute of Technology&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Larva yolk deformation tracking, Cell counting, and Larva heart status monitoring</title>
      <link>https://luoyeguigenno1.github.io/project/some-small-projects/</link>
      <pubDate>Sat, 02 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://luoyeguigenno1.github.io/project/some-small-projects/</guid>
      <description>&lt;h1 id=&#34;larva-yolk-deformation-tracking&#34;&gt;Larva yolk deformation tracking&lt;/h1&gt;
&lt;p&gt;This algorithm uses optical flow to track the deformation of the zebrafish larva yolk, which serves as a visual feedback module for larva yolk soft capture.&lt;/p&gt;














  


&lt;video controls &gt;
  &lt;source src=&#34;2.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;h1 id=&#34;cell-counting&#34;&gt;Cell counting&lt;/h1&gt;
&lt;p&gt;This algorithm automates the task of counting the number of cells in the stem cell culture apparatus. It overcomes the uneven illumination, the low intensity between objects and background, and thin lines interference.
&lt;img src=&#34;1.jpg&#34; alt=&#34;Device and schematic&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;larva-heart-status-monitoring&#34;&gt;Larva heart status monitoring&lt;/h1&gt;
&lt;p&gt;After heart injection, the heart rate is often measured to evaluate the toxicity of certain drugs. This algorithm automates the monitoring process.&lt;/p&gt;














  


&lt;video controls &gt;
  &lt;source src=&#34;1.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;
&lt;p&gt;Supervisor: Prof. &lt;a href=&#34;https://scholar.google.com.hk/citations?user=2DdpHLEAAAAJ&amp;amp;hl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Huijun Gao&lt;/a&gt;,  IEEE Fellow and Professor of Department of Automation, Harbin Institute of Technology&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Zebrafish Larva Morphology Analysis</title>
      <link>https://luoyeguigenno1.github.io/project/zebrafish-larvae-morphology-analysis/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://luoyeguigenno1.github.io/project/zebrafish-larvae-morphology-analysis/</guid>
      <description>&lt;p&gt;The capability to directly deliver agent into the heart inside a single vertebrate is important for understanding organs activities, tissues regeneration, and developing disease models and therapeutic approaches. Despite recent success in exploiting robots to perform injection of cells, embryos, and small animals, it remains challenging to automatically achieve intracardial agent injection due to its inaccessible position, inappropriate immobilization force, and inaccurate localization. Here my labortory develops a high-throughput in vivo heart injection system which enables intracardial agent delivery of a single zebrafish larva in less than fifty seconds. This system not only consistently orients the zebrafish to a pre-designed posture but also provides a controllable capture and injection technique to regulate the deformative injury. Horizontal projection and space height of zebrafish heart are determined with micrometer resolution. To illustrate the power of our system, we have injected several classes of agents and analyzed the results which correlate well with known mechanisms of actions in mammals.&lt;/p&gt;
&lt;p&gt;This video illustrates the workflow of the algorithm here (updated in 07/2019):














  


&lt;video controls &gt;
  &lt;source src=&#34;workflow.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;
&lt;p&gt;The device and schematic of the micromanipulation system is shown in the following picture.
&lt;img src=&#34;1.jpg&#34; alt=&#34;Device and schematic&#34;&gt;&lt;/p&gt;
&lt;p&gt;The objective of zebrafish larva morphology analysis algorithm is to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;detect the larval posture;&lt;/li&gt;
&lt;li&gt;extract its orientation and belly direction;&lt;/li&gt;
&lt;li&gt;give visual feedback to the mechanical device to rotate the object to the desired posture;&lt;/li&gt;
&lt;li&gt;calculate the inhaling position and drive the manipulator to capture the object.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The difficulties inlude&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;uneven illumination;&lt;/li&gt;
&lt;li&gt;noise distractions, such as bubbles, excretion, and small particles, which could cause troubles for extracting the contour of the object;&lt;/li&gt;
&lt;li&gt;morphological variations for different objects with various ages, sizes, contours;&lt;/li&gt;
&lt;li&gt;requirements for high processing speed, which means the visual feedback algorithm needs to achive a bandwith of at least 40 fps.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This algorithm used a range of traditional image processing techniques to achieve robust, accurate, and fast morphology analysis. The workflow is illustrated as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2.jpg&#34; alt=&#34;Workflow&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select the region of interest (ROI)&lt;/li&gt;
&lt;li&gt;Denoise with Gaussian filter and Gaussian filter and Gaussian bilateral filter, where the point is how to cancel the noise without eroding the contour. Confronted with this difficulty, I combine two types of filter.&lt;/li&gt;
&lt;li&gt;Adaptive thresholding to extract the foreground while address the uneven illumination.&lt;/li&gt;
&lt;li&gt;Median filterring and contour extraction.&lt;/li&gt;
&lt;li&gt;Cut the contour into two parts, compare the curvatures of each one to determine the belly side.&lt;/li&gt;
&lt;li&gt;Calculate the direction and the degree needed to rotate the object to the desired posture.&lt;/li&gt;
&lt;li&gt;The mechanical device, a stepper motor, drives the object into the ultimate position with P control.&lt;/li&gt;
&lt;li&gt;Reach the desired podture.&lt;/li&gt;
&lt;li&gt;Adaptive thresholding.&lt;/li&gt;
&lt;li&gt;Median filterring.&lt;/li&gt;
&lt;li&gt;Contour extraction.&lt;/li&gt;
&lt;li&gt;Calculate the inhaling position, painted in a red point.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This algorithms achieves&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;success rate: 98% for larvae aged between 1 dpf to 4 dpf.&lt;/li&gt;
&lt;li&gt;accracy: &amp;lt;3 deg.(orientation) &amp;lt;5 pixels (inhaling point)&lt;/li&gt;
&lt;li&gt;bandwidth: &amp;gt;40 fps&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It works effectively for larvae aged between 1 to 4 dpf with various morphological features.
Furthermore, this project has applied for 4 patents.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Supervisor: Prof. &lt;a href=&#34;https://scholar.google.com.hk/citations?user=2DdpHLEAAAAJ&amp;amp;hl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Huijun Gao&lt;/a&gt;,  IEEE Fellow and Professor of Department of Automation, Harbin Institute of Technology&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
